{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Madhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, time, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Dataset = 16182\n"
     ]
    }
   ],
   "source": [
    "image_path = \"C:/Users/Madhu/Downloads/Images\"\n",
    "captions_english = \"C:/Users/Madhu/OneDrive/Desktop/Sem6/nlp/project/captions_english.txt\"\n",
    "captions_hindi = \"C:/Users/Madhu/OneDrive/Desktop/Sem6/nlp/project/captions_hindi.txt\"\n",
    "captions_malayalam = \"C:/Users/Madhu/OneDrive/Desktop/Sem6/nlp/project/captions_malayalam.txt\"\n",
    "jpgs = os.listdir(image_path)\n",
    "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>captions_english</th>\n",
       "      <th>captions_hindi</th>\n",
       "      <th>captions_malayalam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a child in a pink dress is climbing up a set o...</td>\n",
       "      <td>गुलाबी पोशाक में एक बच्चा प्रवेश मार्ग में सीढ...</td>\n",
       "      <td>പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയി...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a girl going into a wooden building .</td>\n",
       "      <td>एक लड़की लकड़ी की इमारत में जा रही है।</td>\n",
       "      <td>ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing into a wooden playhouse .</td>\n",
       "      <td>एक छोटी लड़की लकड़ी के प्लेहाउस में चढ़ती है।</td>\n",
       "      <td>ഒരു കൊച്ചു പെൺകുട്ടി ഒരു തടി കളിസ്ഥലത്തേക്ക് ക...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing the stairs to her playh...</td>\n",
       "      <td>एक छोटी सी लड़की अपने प्लेहाउस की सीढ़ियाँ चढ़...</td>\n",
       "      <td>ഒരു കൊച്ചു പെൺകുട്ടി തൻ്റെ കളിസ്ഥലത്തേക്കുള്ള ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl in a pink dress going into a woo...</td>\n",
       "      <td>गुलाबी पोशाक में एक छोटी लड़की लकड़ी के केबिन ...</td>\n",
       "      <td>പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കൊച്ചു പെൺകുട്ടി തടി...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                    captions_english  \\\n",
       "0  a child in a pink dress is climbing up a set o...   \n",
       "1              a girl going into a wooden building .   \n",
       "2   a little girl climbing into a wooden playhouse .   \n",
       "3  a little girl climbing the stairs to her playh...   \n",
       "4  a little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                      captions_hindi  \\\n",
       "0  गुलाबी पोशाक में एक बच्चा प्रवेश मार्ग में सीढ...   \n",
       "1             एक लड़की लकड़ी की इमारत में जा रही है।   \n",
       "2      एक छोटी लड़की लकड़ी के प्लेहाउस में चढ़ती है।   \n",
       "3  एक छोटी सी लड़की अपने प्लेहाउस की सीढ़ियाँ चढ़...   \n",
       "4  गुलाबी पोशाक में एक छोटी लड़की लकड़ी के केबिन ...   \n",
       "\n",
       "                                  captions_malayalam  \n",
       "0  പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയി...  \n",
       "1   ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി.  \n",
       "2  ഒരു കൊച്ചു പെൺകുട്ടി ഒരു തടി കളിസ്ഥലത്തേക്ക് ക...  \n",
       "3  ഒരു കൊച്ചു പെൺകുട്ടി തൻ്റെ കളിസ്ഥലത്തേക്കുള്ള ...  \n",
       "4  പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കൊച്ചു പെൺകുട്ടി തടി...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "# Open the text files\n",
    "with open(captions_english, 'r', encoding='utf-8') as file:\n",
    "    text_english = file.read().splitlines()\n",
    "\n",
    "with open(captions_hindi, 'r', encoding='utf-8') as file:\n",
    "    text_hindi = file.read().splitlines()\n",
    "\n",
    "with open(captions_malayalam, 'r', encoding='utf-8') as file:\n",
    "    text_malayalam = file.read().splitlines()\n",
    "\n",
    "# Process the text\n",
    "datatxt = []\n",
    "for line_english, line_hindi, line_malayalam in zip_longest(text_english, text_hindi, text_malayalam, fillvalue=''):\n",
    "    col_english = line_english.split(',')\n",
    "    col_hindi = line_hindi.split(',')\n",
    "    col_malayalam = line_malayalam.split(',')\n",
    "    if len(col_english) != 2 or len(col_hindi) != 2 or len(col_malayalam) != 2:\n",
    "        continue\n",
    "    filename = col_english[0].strip()\n",
    "    captions_english = col_english[1].strip().lower()\n",
    "    captions_hindi = col_hindi[1].strip().lower()\n",
    "    captions_malayalam = col_malayalam[1].strip().lower()\n",
    "\n",
    "    # Add the data to the list\n",
    "    datatxt.append([filename, captions_english, captions_hindi, captions_malayalam])\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame(datatxt, columns=[\"filename\", \"captions_english\", \"captions_hindi\", \"captions_malayalam\"])\n",
    "uni_filenames = data[\"filename\"].unique()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size of english: 8514\n",
      "Vocabulary Size of hindi: 7881\n",
      "Vocabulary Size of malayalam: 18010\n"
     ]
    }
   ],
   "source": [
    "## Vocabulary Size \n",
    "vocabulary_english = []\n",
    "for txt in data.captions_english.values:\n",
    "   vocabulary_english.extend(txt.split())\n",
    "print('Vocabulary Size of english: %d' % len(set(vocabulary_english)))\n",
    "\n",
    "vocabulary_hindi = []\n",
    "for txt in data.captions_hindi.values:\n",
    "   vocabulary_hindi.extend(txt.split())\n",
    "print('Vocabulary Size of hindi: %d' % len(set(vocabulary_hindi)))\n",
    "\n",
    "vocabulary_malayalam=[]\n",
    "for txt in data.captions_malayalam.values:\n",
    "   vocabulary_malayalam.extend(txt.split())\n",
    "print('Vocabulary Size of malayalam: %d' % len(set(vocabulary_malayalam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(text_original):\n",
    "   text_no_punctuation = text_original.translate(string.punctuation)\n",
    "   return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "   text_len_more_than1 = \"\"\n",
    "   for word in text.split():\n",
    "       if len(word) > 1:\n",
    "           text_len_more_than1 += \" \" + word\n",
    "   return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "   text_no_numeric = \"\"\n",
    "   for word in text.split():\n",
    "       isalpha = word.isalpha()\n",
    "       if isalpha:\n",
    "           text_no_numeric += \" \" + word\n",
    "   return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "   text = remove_punctuation(text_original)\n",
    "   text = remove_single_character(text)\n",
    "   text = remove_numeric(text)\n",
    "   return(text)\n",
    "\n",
    "for i, caption in enumerate(data.captions_english.values):\n",
    "   newcaption = text_clean(caption)\n",
    "   data[\"captions_english\"].iloc[i] = newcaption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding tags <start> <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയിൽ ഒരു കൂട്ടം പടികൾ കയറുന്നു. <end>',\n",
       " '<start> ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി. <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions_english = []\n",
    "for caption  in data[\"captions_english\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_english.append(caption)\n",
    "\n",
    "all_captions_english[:10]\n",
    "\n",
    "\n",
    "all_captions_hindi = []\n",
    "for caption  in data[\"captions_hindi\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_hindi.append(caption)\n",
    "\n",
    "all_captions_hindi[:2]\n",
    "\n",
    "\n",
    "all_captions_malayalam = []\n",
    "for caption  in data[\"captions_malayalam\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_malayalam.append(caption)\n",
    "\n",
    "all_captions_malayalam[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_name_vector = []\n",
    "for annot in data[\"filename\"]:\n",
    "   full_image_path = image_path +\"/\"+ annot\n",
    "   all_img_name_vector.append(full_image_path)\n",
    "\n",
    "all_img_name_vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_img_name_vector) : 37212\n",
      "len(all_captions_english) : 37212\n",
      "len(all_captions_hindi) : 37212\n",
      "len(all_captions_malayalam) : 37212\n",
      "{111636}\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n",
    "print(f\"len(all_captions_english) : {len(all_captions_english)}\")\n",
    "print(f\"len(all_captions_hindi) : {len(all_captions_hindi)}\")\n",
    "print(f\"len(all_captions_malayalam) : {len(all_captions_malayalam)}\")\n",
    "all_captions_size = {len(all_captions_english)+len(all_captions_hindi)+len(all_captions_malayalam)}\n",
    "print(all_captions_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_captions_english: 7680\n",
      "Length of train_captions_hindi: 7680\n",
      "Length of train_captions_malayalam: 7680\n",
      "Length of img_name_vector: 7680\n",
      "Example img_name_vector: ['C:/Users/Madhu/Downloads/Images/2120469056_7a738413be.jpg', 'C:/Users/Madhu/Downloads/Images/2842609837_b3a0b383f7.jpg', 'C:/Users/Madhu/Downloads/Images/3470129475_9e58b6742c.jpg', 'C:/Users/Madhu/Downloads/Images/167295035_336f5f5f27.jpg', 'C:/Users/Madhu/Downloads/Images/159712188_d530dd478c.jpg']\n"
     ]
    }
   ],
   "source": [
    "def data_limiter(num, total_captions_english, total_captions_hindi, total_captions_malayalam, all_img_name_vector):\n",
    "    # Shuffle the captions and image names\n",
    "    train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector = shuffle(\n",
    "        total_captions_english, total_captions_hindi, total_captions_malayalam, all_img_name_vector, random_state=1\n",
    "    )\n",
    "    \n",
    "    # Limit the data to the specified number\n",
    "    train_captions_english = train_captions_english[:num]\n",
    "    train_captions_hindi = train_captions_hindi[:num]\n",
    "    train_captions_malayalam = train_captions_malayalam[:num]\n",
    "    img_name_vector = img_name_vector[:num]\n",
    "    \n",
    "    print(\"Length of train_captions_english:\", len(train_captions_english))\n",
    "    print(\"Length of train_captions_hindi:\", len(train_captions_hindi))\n",
    "    print(\"Length of train_captions_malayalam:\", len(train_captions_malayalam))\n",
    "    print(\"Length of img_name_vector:\", len(img_name_vector))\n",
    "    print(\"Example img_name_vector:\", img_name_vector[:5])  # Print first 5 elements for inspection\n",
    "    \n",
    "    return train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector\n",
    "\n",
    "train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector = data_limiter(7680, all_captions_english, all_captions_hindi, all_captions_malayalam, all_img_name_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - VGG16 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Madhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Madhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_jpeg(img, channels=3)\n",
    "   img = tf.image.resize(img, (224, 224))\n",
    "   img = preprocess_input(img)\n",
    "   return img, image_path\n",
    "\n",
    "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "encode_train = sorted(set(img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "print(image_dataset)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "print(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#%%time\\nfor img, path in tqdm(image_dataset):\\n batch_features = image_features_extract_model(img)\\n batch_features = tf.reshape(batch_features,\\n                             (batch_features.shape[0], -1, batch_features.shape[3]))\\n\\n for bf, p in zip(batch_features, path):\\n   path_of_feature = p.numpy().decode(\"utf-8\")\\n   np.save(path_of_feature, bf.numpy())\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#%%time\n",
    "for img, path in tqdm(image_dataset):\n",
    " batch_features = image_features_extract_model(img)\n",
    " batch_features = tf.reshape(batch_features,\n",
    "                             (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    " for bf, p in zip(batch_features, path):\n",
    "   path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "   np.save(path_of_feature, bf.numpy())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=5000\n",
    "# Tokenize English captions\n",
    "tokenizer_english = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                           oov_token=\"<unk>\",\n",
    "                                                           filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_english.fit_on_texts(train_captions_english)\n",
    "train_seqs_english = tokenizer_english.texts_to_sequences(train_captions_english)\n",
    "tokenizer_english.word_index['<pad>'] = 0\n",
    "tokenizer_english.index_word[0] = '<pad>'\n",
    "cap_vector_english = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_english, padding='post')\n",
    "\n",
    "# Tokenize Hindi captions\n",
    "tokenizer_hindi = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                         oov_token=\"<unk>\",\n",
    "                                                         filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_hindi.fit_on_texts(train_captions_hindi)\n",
    "train_seqs_hindi = tokenizer_hindi.texts_to_sequences(train_captions_hindi)\n",
    "tokenizer_hindi.word_index['<pad>'] = 0\n",
    "tokenizer_hindi.index_word[0] = '<pad>'\n",
    "cap_vector_hindi = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_hindi, padding='post')\n",
    "\n",
    "# Tokenize Malayalam captions\n",
    "tokenizer_malayalam = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                             oov_token=\"<unk>\",\n",
    "                                                             filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_malayalam.fit_on_texts(train_captions_malayalam)\n",
    "train_seqs_malayalam = tokenizer_malayalam.texts_to_sequences(train_captions_malayalam)\n",
    "tokenizer_malayalam.word_index['<pad>'] = 0\n",
    "tokenizer_malayalam.index_word[0] = '<pad>'\n",
    "cap_vector_malayalam = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_malayalam, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Captions:\n",
      "<start>  two children swimming in green water <end>\n",
      "<start>  group of four friends are drinking alcohol at party <end>\n",
      "<start>  man jumps his bicycle high above cement incline <end>\n",
      "\n",
      "Hindi Captions:\n",
      "<start> दो बच्चे हरे पानी में तैर रहे हैं। <end>\n",
      "<start> चार दोस्तों का एक समूह एक पार्टी में शराब पी रहा है। <end>\n",
      "<start> एक आदमी अपनी साइकिल को सीमेंट की ढलान से ऊपर कूदता है। <end>\n",
      "\n",
      "Malayalam Captions:\n",
      "<start> രണ്ട് കുട്ടികൾ പച്ചവെള്ളത്തിൽ നീന്തുന്നു. <end>\n",
      "<start> നാല് സുഹൃത്തുക്കളുടെ ഒരു സംഘം ഒരു പാർട്ടിയിൽ മദ്യം കഴിക്കുന്നു. <end>\n",
      "<start> ഒരു മനുഷ്യൻ തൻ്റെ സൈക്കിൾ സിമൻ്റ് ചരിവിന് മുകളിൽ ചാടുന്നു. <end>\n"
     ]
    }
   ],
   "source": [
    "# English captions\n",
    "print(\"English Captions:\")\n",
    "for caption in train_captions_english[:3]:\n",
    "    print(caption)\n",
    "\n",
    "# Hindi captions\n",
    "print(\"\\nHindi Captions:\")\n",
    "for caption in train_captions_hindi[:3]:\n",
    "    print(caption)\n",
    "\n",
    "# Malayalam captions\n",
    "print(\"\\nMalayalam Captions:\")\n",
    "for caption in train_captions_malayalam[:3]:\n",
    "    print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three tokenized sequences (English):\n",
      "[2, 13, 56, 132, 4, 60, 24, 3]\n",
      "[2, 55, 12, 100, 731, 17, 438, 2203, 21, 616, 3]\n",
      "[2, 11, 68, 32, 130, 182, 256, 617, 1652, 3]\n",
      "\n",
      "First three tokenized sequences (Hindi):\n",
      "[3, 16, 50, 103, 32, 7, 286, 21, 10, 4]\n",
      "[3, 123, 1072, 22, 2, 74, 2, 616, 7, 402, 274, 9, 5, 4]\n",
      "[3, 2, 15, 70, 78, 20, 656, 12, 479, 14, 90, 62, 5, 4]\n",
      "\n",
      "First three tokenized sequences (Malayalam):\n",
      "[3, 7, 37, 2028, 206, 4]\n",
      "[3, 73, 3027, 2, 417, 2, 944, 3028, 241, 4]\n",
      "[3, 2, 8, 69, 49, 1258, 2029, 65, 18, 4]\n"
     ]
    }
   ],
   "source": [
    "# English captions\n",
    "print(\"First three tokenized sequences (English):\")\n",
    "for seq in train_seqs_english[:3]:\n",
    "    print(seq)\n",
    "\n",
    "# Hindi captions\n",
    "print(\"\\nFirst three tokenized sequences (Hindi):\")\n",
    "for seq in train_seqs_hindi[:3]:\n",
    "    print(seq)\n",
    "\n",
    "# Malayalam captions\n",
    "print(\"\\nFirst three tokenized sequences (Malayalam):\")\n",
    "for seq in train_seqs_malayalam[:3]:\n",
    "    print(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "3\n",
      "Max Length of english caption : Min Length of english caption = 58 : 3\n",
      "104\n",
      "4\n",
      "Max Length of hindi caption : Min Length of hindi caption = 104 : 4\n",
      "58\n",
      "3\n",
      "Max Length of malayalam caption : Min Length of malayalam caption = 58 : 3\n"
     ]
    }
   ],
   "source": [
    "def calc_max_length(tensor):\n",
    "   return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def calc_min_length(tensor):\n",
    "   return min(len(t) for t in tensor)\n",
    "\n",
    "max_length_english = calc_max_length(train_seqs_malayalam)\n",
    "min_length_malayalam = calc_min_length(train_seqs_malayalam)\n",
    "\n",
    "print(max_length_english)\n",
    "print(min_length_malayalam)\n",
    "print('Max Length of english caption : Min Length of english caption = '+ str(max_length_english) +\" : \"+str(min_length_malayalam))\n",
    "\n",
    "max_length_hindi = calc_max_length(train_seqs_hindi)\n",
    "min_length_hindi = calc_min_length(train_seqs_hindi)\n",
    "\n",
    "print(max_length_hindi)\n",
    "print(min_length_hindi)\n",
    "print('Max Length of hindi caption : Min Length of hindi caption = '+ str(max_length_hindi) +\" : \"+str(min_length_hindi))\n",
    "\n",
    "max_length_malayalam = calc_max_length(train_seqs_malayalam)\n",
    "min_length_malayalam = calc_min_length(train_seqs_malayalam)\n",
    "\n",
    "print(max_length_malayalam)\n",
    "print(min_length_malayalam)\n",
    "print('Max Length of malayalam caption : Min Length of malayalam caption = '+ str(max_length_malayalam) +\" : \"+str(min_length_malayalam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train_english, cap_val_english = train_test_split(img_name_vector,cap_vector_english, test_size=0.2, random_state=0)\n",
    "img_name_train, img_name_val, cap_train_hindi, cap_val_hindi = train_test_split(img_name_vector,cap_vector_hindi, test_size=0.2, random_state=0)\n",
    "img_name_train, img_name_val, cap_train_malayalam, cap_val_malayalam = train_test_split(img_name_vector,cap_vector_malayalam, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size_english = len(tokenizer_english.word_index) + 1\n",
    "vocab_size_hindi = len(tokenizer_hindi.word_index) + 1\n",
    "vocab_size_malayalam = len(tokenizer_malayalam.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "features_shape = 512\n",
    "attention_features_shape = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_func(img_name, cap):\n",
    " img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    " return img_tensor, cap\n",
    "\n",
    "dataset_english = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_english))\n",
    "dataset_english = dataset_english.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_english = dataset_english.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_english = dataset_english.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "dataset_hindi = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_hindi))\n",
    "dataset_hindi = dataset_hindi.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_hindi = dataset_hindi.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_hindi = dataset_hindi.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset_malayalam = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_malayalam))\n",
    "dataset_malayalam = dataset_malayalam.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_malayalam = dataset_malayalam.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_malayalam = dataset_malayalam.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_size = len(vocabulary_english)\n",
    "hindi_vocab_size = len(vocabulary_hindi)\n",
    "malayalam_vocab_size = len(vocabulary_malayalam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "   from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    " loss_ = loss_object(real, pred)\n",
    " mask = tf.cast(mask, dtype=loss_.dtype)\n",
    " loss_ *= mask\n",
    "\n",
    " return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_Encoder(tf.keras.Model):\n",
    "   # This encoder passes the features through a Fully connected layer\n",
    "   def __init__(self, embedding_dim):\n",
    "       super(VGG16_Encoder, self).__init__()\n",
    "       # shape after fc == (batch_size, 49, embedding_dim)\n",
    "       self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "\n",
    "   def call(self, x):\n",
    "       #x= self.dropout(x)\n",
    "       x = self.fc(x)\n",
    "       x = tf.nn.relu(x)\n",
    "       return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn_Local_Decoder(tf.keras.Model):\n",
    " def __init__(self, embedding_dim, units, vocab_size):\n",
    "   super(Rnn_Local_Decoder, self).__init__()\n",
    "   self.units = units\n",
    "   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "   self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                  return_sequences=True,\n",
    "                                  return_state=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "  \n",
    "   self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "\n",
    "   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "   self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "   self.Uattn = tf.keras.layers.Dense(units)\n",
    "   self.Wattn = tf.keras.layers.Dense(units)\n",
    "   self.Vattn = tf.keras.layers.Dense(1)\n",
    "\n",
    " def call(self, x, features, hidden):\n",
    "   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
    "   attention_weights = tf.nn.softmax(score, axis=1)\n",
    "   context_vector = attention_weights * features\n",
    "   context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "   x = self.embedding(x)\n",
    "   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "   output, state = self.gru(x)\n",
    "   x = self.fc1(output)\n",
    "   x = tf.reshape(x, (-1, x.shape[2]))\n",
    "   x= self.dropout(x)\n",
    "   x= self.batchnormalization(x)\n",
    "   x = self.fc2(x)\n",
    "   return x, state, attention_weights\n",
    "\n",
    " def reset_state(self, batch_size):\n",
    "   return tf.zeros((batch_size, self.units))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VGG16_Encoder(embedding_dim)\n",
    "decoder_english = Rnn_Local_Decoder(embedding_dim, units, vocab_size_english)\n",
    "decoder_hindi = Rnn_Local_Decoder(embedding_dim, units, vocab_size_hindi)\n",
    "decoder_malayalam = Rnn_Local_Decoder(embedding_dim, units, vocab_size_malayalam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target,decoder,tokenizer):\n",
    " loss = 0\n",
    "\n",
    "\n",
    " hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    " dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    " with tf.GradientTape() as tape:\n",
    "     features = encoder(img_tensor)\n",
    "     for i in range(1, target.shape[1]):\n",
    "         # passing the features through the decoder\n",
    "         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "         loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "         # using teacher forcing\n",
    "         dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    " total_loss = (loss / int(target.shape[1]))\n",
    " trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    " gradients = tape.gradient(loss, trainable_variables)\n",
    " optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    " return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image,max_length,decoder,tokenizer):\n",
    "   attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "   hidden = decoder.reset_state(batch_size=1)\n",
    "   temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "   img_tensor_val = image_features_extract_model(temp_input)\n",
    "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "   features = encoder(img_tensor_val)\n",
    "   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "   result = []\n",
    "\n",
    "   for i in range(max_length):\n",
    "       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "       predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "       result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "       if tokenizer.index_word[predicted_id] == '<end>':\n",
    "           return result, attention_plot\n",
    "\n",
    "       dec_input = tf.expand_dims([predicted_id], 0)\n",
    "   attention_plot = attention_plot[:len(result), :]\n",
    "\n",
    "   return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "   temp_image = np.array(Image.open(image))\n",
    "   fig = plt.figure(figsize=(10, 10))\n",
    "   len_result = len(result)\n",
    "   for l in range(len_result):\n",
    "       temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "       ax.set_title(result[l])\n",
    "       img = ax.imshow(temp_image)\n",
    "       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot_english = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 1\n",
    "# Define a checkpoint directory and a checkpoint manager\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder_english)\n",
    "\n",
    "# Define a checkpoint manager\n",
    "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target_english)) in enumerate(dataset_english):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target_english,decoder_english,tokenizer_english)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target_english.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss / num_steps))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "\n",
    "    manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training with the already saved checkpoints\n",
    "\n",
    "# # Define the number of epochs to train\n",
    "# EPOCHS = 1 # Change this to the number of additional epochs you want to train\n",
    "\n",
    "# # Define a checkpoint directory and a checkpoint manager\n",
    "# checkpoint_dir = './training_checkpoints'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder_english)\n",
    "\n",
    "# # Define a checkpoint manager\n",
    "# manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "# # Restore the latest checkpoint if available\n",
    "# start_epoch = 0\n",
    "# if manager.latest_checkpoint:\n",
    "#     start_epoch = int(manager.latest_checkpoint.split('-')[-1])\n",
    "#     checkpoint.restore(manager.latest_checkpoint)\n",
    "#     print(f'Restored checkpoint from epoch {start_epoch}')\n",
    "\n",
    "# # Initialize loss plot\n",
    "# loss_plot_english = []\n",
    "\n",
    "# # Continue training from the next epoch after the last saved checkpoint\n",
    "# for epoch in range(start_epoch, start_epoch + EPOCHS):\n",
    "#     start = time.time()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for (batch, (img_tensor, target_english)) in enumerate(dataset_english):\n",
    "#         batch_loss, t_loss = train_step(img_tensor, target_english, decoder_english, tokenizer_english)\n",
    "#         total_loss += t_loss\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#                 epoch + 1, batch, batch_loss.numpy() / int(target_english.shape[1])))\n",
    "\n",
    "#     # Store the epoch end loss value to plot later\n",
    "#     loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "#     print('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss / num_steps))\n",
    "#     print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "#     # Save the checkpoint at the end of each epoch\n",
    "#     manager.save()\n",
    "\n",
    "# # Plot the loss curve\n",
    "# plt.plot(loss_plot_english)\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Curve')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot_english)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot English')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer_english.index_word[i] for i in cap_val_english[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "   if i==\"<unk>\":\n",
    "       result.remove(i)\n",
    "\n",
    "#remove <end> from result        \n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "\n",
    "plot_attention(image, result, attention_plot)\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "\n",
    "Image.open(img_name_val[rid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hindi Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot_hindi = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target_hindi)) in enumerate(dataset_hindi):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target_hindi,decoder_hindi,tokenizer_hindi)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target_hindi.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss / num_steps))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer_hindi.index_word[i] for i in cap_val_hindi[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "   if i==\"<unk>\":\n",
    "       result.remove(i)\n",
    "\n",
    "#remove <end> from result        \n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "\n",
    "plot_attention(image, result, attention_plot)\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "\n",
    "Image.open(img_name_val[rid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malayalam Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot_malayalam = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target_malayalam)) in enumerate(dataset_malayalam):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target_malayalam,decoder_malayalam,tokenizer_malayalam)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target_hindi.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss / num_steps))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "start = time.time()\n",
    "real_caption = ' '.join([tokenizer_malayalam.index_word[i] for i in cap_val_malayalam[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "first = real_caption.split(' ', 1)[1]\n",
    "real_caption = first.rsplit(' ', 1)[0]\n",
    "\n",
    "#remove \"<unk>\" in result\n",
    "for i in result:\n",
    "   if i==\"<unk>\":\n",
    "       result.remove(i)\n",
    "\n",
    "#remove <end> from result        \n",
    "result_join = ' '.join(result)\n",
    "result_final = result_join.rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = result_final\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', result_final)\n",
    "\n",
    "plot_attention(image, result, attention_plot)\n",
    "print(f\"time took to Predict: {round(time.time()-start)} sec\")\n",
    "\n",
    "Image.open(img_name_val[rid])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
